"""
多模态情感分析深度学习系统
实现了一个结合文本、音频和视觉特征的端到端深度学习模型
用于精确预测人类情感状态

作者: [您的姓名]
机构: [您的机构]
日期: [日期]
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, BertTokenizer
from torchvision.models import resnet50
import librosa
import numpy as np
from typing import Tuple, Dict, Optional

class MultimodalAttentionFusion(nn.Module):
    """
    创新的多模态注意力融合层
    实现跨模态特征交互和动态权重分配
    
    参数:
        text_dim: 文本特征维度
        audio_dim: 音频特征维度
        visual_dim: 视觉特征维度
        hidden_dim: 注意力隐藏层维度
        dropout: dropout率
    """
    def __init__(self, text_dim: int, audio_dim: int, visual_dim: int, hidden_dim: int = 256, dropout: float = 0.2):
        super().__init__()
        self.text_proj = nn.Linear(text_dim, hidden_dim)
        self.audio_proj = nn.Linear(audio_dim, hidden_dim)
        self.visual_proj = nn.Linear(visual_dim, hidden_dim)
        
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, dropout=dropout)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(dropout)
        
        # 门控融合机制
        self.gate = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim),
            nn.Sigmoid()
        )
        
    def forward(self, text_feat: torch.Tensor, audio_feat: torch.Tensor, visual_feat: torch.Tensor) -> torch.Tensor:
        # 特征投影到共同空间
        text_proj = self.text_proj(text_feat)
        audio_proj = self.audio_proj(audio_feat)
        visual_proj = self.visual_proj(visual_feat)
        
        # 拼接所有特征作为注意力输入
        combined = torch.stack([text_proj, audio_proj, visual_proj], dim=1)
        
        # 跨模态注意力
        attn_output, _ = self.attention(combined, combined, combined)
        attn_output = self.layer_norm(combined + self.dropout(attn_output))
        
        # 门控融合
        gates = self.gate(torch.cat([text_proj, audio_proj, visual_proj], dim=-1))
        fused = (gates * text_proj + (1 - gates) * torch.cat([audio_proj, visual_proj], dim=-1))
        
        return fused

class MultimodalEmotionClassifier(nn.Module):
    """
    多模态情感分类器
    整合BERT(文本)、CNN(视觉)和LSTM(音频)特征
    
    参数:
        num_classes: 情感类别数
        text_model: 预训练文本模型名称
        audio_feat_dim: 音频特征维度
        dropout: dropout率
    """
    def __init__(self, num_classes: int = 7, text_model: str = 'bert-base-uncased', 
                 audio_feat_dim: int = 128, dropout: float = 0.3):
        super().__init__()
        
        # 文本特征提取器
        self.bert = BertModel.from_pretrained(text_model)
        self.text_fc = nn.Linear(self.bert.config.hidden_size, 256)
        
        # 音频特征提取器
        self.audio_lstm = nn.LSTM(input_size=audio_feat_dim, hidden_size=128, 
                                 num_layers=2, bidirectional=True, batch_first=True)
        self.audio_fc = nn.Linear(256, 128)
        
        # 视觉特征提取器
        self.resnet = resnet50(pretrained=True)
        self.visual_fc = nn.Linear(2048, 256)
        
        # 多模态融合
        self.fusion = MultimodalAttentionFusion(text_dim=256, audio_dim=128, visual_dim=256)
        
        # 分类器
        self.classifier = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, num_classes)
        )
        
        # 初始化权重
        self._init_weights()
        
    def _init_weights(self):
        """自定义权重初始化"""
        for module in [self.text_fc, self.audio_fc, self.visual_fc]:
            nn.init.xavier_normal_(module.weight)
            nn.init.constant_(module.bias, 0)
        
    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, 
                audio_features: torch.Tensor, visual_features: torch.Tensor) -> torch.Tensor:
        # 文本特征提取
        text_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_feat = self.text_fc(text_outputs.last_hidden_state[:, 0, :])  # 取[CLS] token
        
        # 音频特征提取
        audio_output, _ = self.audio_lstm(audio_features)
        audio_feat = self.audio_fc(audio_output[:, -1, :])  # 取最后一个时间步
        
        # 视觉特征提取
        visual_feat = self.resnet(visual_features)
        visual_feat = self.visual_fc(visual_feat)
        
        # 多模态融合
        fused_feat = self.fusion(text_feat, audio_feat, visual_feat)
        
        # 情感分类
        logits = self.classifier(fused_feat)
        
        return logits

class EmotionDataset(torch.utils.data.Dataset):
    """
    自定义多模态情感数据集加载器
    处理文本、音频和视觉数据的对齐和预处理
    """
    def __init__(self, text_data: list, audio_paths: list, visual_paths: list, labels: list, 
                 tokenizer: BertTokenizer, sample_rate: int = 22050, max_length: int = 128):
        self.text_data = text_data
        self.audio_paths = audio_paths
        self.visual_paths = visual_paths
        self.labels = labels
        self.tokenizer = tokenizer
        self.sample_rate = sample_rate
        self.max_length = max_length
        
    def __len__(self) -> int:
        return len(self.labels)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # 文本处理
        text = self.text_data[idx]
        inputs = self.tokenizer(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors="pt")
        
        # 音频处理 (提取MFCC特征)
        audio, _ = librosa.load(self.audio_paths[idx], sr=self.sample_rate)
        mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=13)
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)
        audio_feat = np.concatenate([mfcc, delta, delta2], axis=0).T  # (time, 39)
        
        # 视觉处理 (简化示例，实际应从图像文件加载)
        visual_feat = torch.load(self.visual_paths[idx])
        
        return {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'audio_features': torch.FloatTensor(audio_feat),
            'visual_features': visual_feat,
            'labels': torch.LongTensor([self.labels[idx]])
        }

def collate_fn(batch: list) -> Dict[str, torch.Tensor]:
    """
    自定义批处理函数，处理变长音频序列
    """
    # 文本数据
    input_ids = torch.stack([item['input_ids'] for item in batch])
    attention_mask = torch.stack([item['attention_mask'] for item in batch])
    
    # 音频数据 (填充到最大长度)
    audio_features = [item['audio_features'] for item in batch]
    audio_lengths = [len(feat) for feat in audio_features]
    max_audio_len = max(audio_lengths)
    audio_padded = torch.zeros(len(batch), max_audio_len, audio_features[0].shape[-1])
    for i, feat in enumerate(audio_features):
        audio_padded[i, :len(feat)] = feat
    
    # 视觉数据
    visual_features = torch.stack([item['visual_features'] for item in batch])
    
    # 标签
    labels = torch.cat([item['labels'] for item in batch])
    
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'audio_features': audio_padded,
        'visual_features': visual_features,
        'labels': labels,
        'audio_lengths': torch.LongTensor(audio_lengths)
    }

class EarlyStopping:
    """
    早停机制，防止过拟合
    """
    def __init__(self, patience: int = 5, delta: float = 0, verbose: bool = False):
        self.patience = patience
        self.delta = delta
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = np.Inf
        
    def __call__(self, val_loss: float, model: nn.Module):
        score = -val_loss
        
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0
            
    def save_checkpoint(self, val_loss: float, model: nn.Module):
        """保存最佳模型"""
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model...')
        torch.save(model.state_dict(), 'checkpoint.pt')
        self.val_loss_min = val_loss

def train_model(model: nn.Module, train_loader: torch.utils.data.DataLoader, 
               val_loader: torch.utils.data.DataLoader, epochs: int = 50, 
               lr: float = 2e-5, device: str = 'cuda') -> Tuple[nn.Module, list]:
    """
    模型训练与验证循环
    
    参数:
        model: 要训练的模型
        train_loader: 训练数据加载器
        val_loader: 验证数据加载器
        epochs: 训练轮数
        lr: 学习率
        device: 计算设备
    
    返回:
        tuple: (最佳模型, 训练指标历史)
    """
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)
    criterion = nn.CrossEntropyLoss()
    early_stopping = EarlyStopping(patience=5, verbose=True)
    
    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
    
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        for batch in train_loader:
            optimizer.zero_grad()
            
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'audio_lengths'}
            outputs = model(input_ids=inputs['input_ids'], 
                           attention_mask=inputs['attention_mask'],
                           audio_features=inputs['audio_features'],
                           visual_features=inputs['visual_features'])
            
            loss = criterion(outputs, inputs['labels'])
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_loss += loss.item() * inputs['input_ids'].size(0)
        
        # 验证阶段
        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)
        train_loss = train_loss / len(train_loader.dataset)
        
        # 记录历史
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        # 学习率调整
        scheduler.step(val_loss)
        
        # 早停检查
        early_stopping(val_loss, model)
        if early_stopping.early_stop:
            print("Early stopping triggered")
            break
            
        print(f'Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}')
    
    # 加载最佳模型
    model.load_state_dict(torch.load('checkpoint.pt'))
    return model, history

def evaluate_model(model: nn.Module, data_loader: torch.utils.data.DataLoader, 
                  criterion: nn.Module, device: str = 'cuda') -> Tuple[float, float]:
    """
    模型评估函数
    
    返回:
        tuple: (平均损失, 准确率)
    """
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch in data_loader:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'audio_lengths'}
            outputs = model(input_ids=inputs['input_ids'], 
                          attention_mask=inputs['attention_mask'],
                          audio_features=inputs['audio_features'],
                          visual_features=inputs['visual_features'])
            
            loss = criterion(outputs, inputs['labels'])
            total_loss += loss.item() * inputs['input_ids'].size(0)
            
            _, predicted = torch.max(outputs.data, 1)
            total += inputs['labels'].size(0)
            correct += (predicted == inputs['labels']).sum().item()
    
    avg_loss = total_loss / len(data_loader.dataset)
    accuracy = correct / total
    return avg_loss, accuracy

if __name__ == '__main__':
    # 示例使用代码
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # 初始化组件
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    model = MultimodalEmotionClassifier(num_classes=7).to(device)
    
    # 示例数据 (实际应用中应从文件加载)
    text_data = ["I'm feeling great today!", "This is so frustrating..."]
    audio_paths = ["audio1.wav", "audio2.wav"]  # 假设这些文件存在
    visual_paths = ["visual1.pt", "visual2.pt"]  # 假设这些文件存在
    labels = [0, 3]  # 假设0=快乐, 3=愤怒
    
    # 创建数据集和数据加载器
    dataset = EmotionDataset(text_data, audio_paths, visual_paths, labels, tokenizer)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    
    # 分割训练集和验证集
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)
    
    # 训练模型
    trained_model, history = train_model(model, train_loader, val_loader, epochs=20, lr=2e-5, device=device)
    
    # 保存模型
    torch.save(trained_model.state_dict(), 'multimodal_emotion_model.pt')
